2025-09-29
- Implement recursive splitting for both redaction and rephrase phases. When providers return refusal/policy messages or partial outputs, batches are split into smaller subsets and resubmitted until sufficient coverage is achieved or a split limit is reached.
- Redaction spans mode participates in splitting; partial arrays are merged across splits.
- For full_text outputs, partial arrays parsed by `normalizeBatchTextParsed` are converted into synthetic ITEM-tagged blocks and merged across splits for reliable alignment.
- No API changes. Logs tagged with [RUNID-BATCH] and recursive splitting is encapsulated per group/model.

2025-09-15
- Respect `phase` parameter when invoking AI calls. When `phase` is `redaction`, the function will no longer initiate any rephrase calls. When `phase` is `rephrase`, the function will skip redaction calls. When `phase` is `both`, execution remains two-phased internally with rephrase only starting after all redactions across models have completed. This prevents rephrase calls from starting early when the client orchestrates phases in separate invocations.
- Added explicit logs indicating when a phase is skipped due to the requested `phase`.

Files impacted:
- supabase/functions/post-process-comments/index.ts

2025-09-12
- Fix alignment bug for ID-tagged AI outputs: when mapping redaction/rephrase results back to comments, coerce `originalRow`/`scannedIndex` to numbers before comparison. This prevents misses when IDs arrive as strings (e.g., "10") while comments hold numbers (10), which could show original text instead of redacted for some rows.
- Frontend merge now preserves backend `finalText` and `mode` fields so UI can reliably display AI-processed text even if `redactedText`/`rephrasedText` are omitted in some routes.

- Respect RPM: serialize redaction and rephrase calls per model/group and use the actual group's `rpm_limit` for pacing via `enforceRpmDelay`. This avoids parallel hits to the same model that could trigger "Too Many Requests" on Bedrock Anthropic Claude (haiku).

- Remove hard caps on batch size: Removed both `DEFAULT_POST_PROCESS_BATCH_SIZE = 100` limit and provider/model-specific hard caps (GPT-4o-mini: 12, Claude Haiku: 10). Batch size is now determined purely by token limits and RPM constraints, allowing larger batches when token capacity permits.

Files impacted:
- supabase/functions/post-process-comments/index.ts
- src/components/CommentEditor.tsx
# 2025-09-13
- Ensure fallback redactions are surfaced to UI: when AI redaction output is empty but the deterministic policy redaction changes the text, include `redactedText` in the response. This fixes cases where the final column could show the original text due to missing `redactedText` despite a redacted `finalText`.
- No change to `rephrasedText` behavior: still only included when AI produces a rephrase (to avoid echoing originals).

Files impacted:
- supabase/functions/post-process-comments/index.ts
 
## 2025-09-13 (later)
- Prevent "pending" AI logs on last batches by adding a dynamic timeout derived from remaining function time, passed into `callAI` (Azure/OpenAI/Bedrock). Avoids edge cap termination without response logging.
- Fix ID realignment to coerce `originalRow`/`scannedIndex` to numbers and avoid skipping when values are 0-like; reliably maps `<<<ITEM k>>>` outputs back to correct rows.
- Relax mode gating so redaction/rephrase apply when either `identifiable` or `concerning` is true, preventing misses where the preferred mode is unavailable and the other phase's output exists.

### Include both AI outputs regardless of selected mode
- `processedComments` now includes `redactedText` and `rephrasedText` whenever they differ from the original text, regardless of which mode produced the `finalText`. This ensures the UI can always switch to Redact/Rephrase and display the correct AI-generated alternative (fixes cases like Row 1 in run 1040).

Files impacted:
- supabase/functions/post-process-comments/index.ts
 
2025-09-15
- Add optional redaction spans mode to reduce output tokens. When `scanConfig.redaction_output_mode = "spans"`, the model returns JSON objects per `<<<ITEM k>>>` with `index` and `redact` substrings. The server applies spans to the original text using layered matching (literal, case-insensitive, whitespace-normalized) and then enforces deterministic policy. Fallback to policy-only if JSON cannot be parsed.
- New optional `scanConfig.span_min_length` (default 2) filters ultra-short spans to avoid over-redaction.

Files impacted:
- supabase/functions/post-process-comments/index.ts
# Post-Process Comments Edge Function - Recent Changes

## Latest Updates

### Enhanced Content Transformation
- **AI-Powered Redaction**: Improved AI models for intelligent content redaction
- **Smart Rephrasing**: Better preservation of meaning while ensuring anonymity
- **Policy Enforcement**: Enhanced automatic detection and replacement of sensitive content
- **Consistent Output**: Better formatting and structure preservation

### Improved Batch Processing
- **I/O Ratio Optimization**: Batch sizing now uses per-phase I/O ratios from `batch_sizing_config` (`redaction_io_ratio`, `rephrase_io_ratio`).
- **Token-Aware Greedy Chunking**: Implemented greedy chunk builder that respects input token limits (minus prompt reserve) and output token limits per phase, splitting chunks when needed.
- **Dual-Phase Guard**: When both redaction and rephrase are requested, chunks are further split to ensure each phase stays within its output token limit.
- **Safety Margins**: Configurable safety margin percentage applied to calculated batch sizes and to conservative input/output limits and AI call `max_tokens` to reduce overrun risk.
- **Large Dataset Support**: Improved handling of large comment volumes with stable chunking.
- **Timeout Protection**: Standardized timeouts via `POSTPROCESS_AI_REQUEST_TIMEOUT_MS` (default 140000 ms) and `POSTPROCESS_BEDROCK_REQUEST_TIMEOUT_MS` (default 140000 ms).

### Enhanced AI Integration
- **Multiple Provider Support**: Enhanced support for OpenAI and Azure OpenAI
- **Model Configuration**: Uses same configuration system as other functions
- **Temperature Control**: Support for configurable temperature settings
- **Token Limit Optimization**: Better token usage and limit management

### Policy Enforcement Improvements
- **Job Level Detection**: Enhanced detection of job levels, grades, and bands
- **Tenure Redaction**: Better identification and replacement of tenure statements
- **Role Identifier Removal**: Improved detection of role and position identifiers
- **Consistent Placeholders**: Standardized "XXXX" usage across all redactions

### Error Handling Enhancements
- **Graceful Degradation**: Continues processing even when some AI calls fail
- **Fallback Mechanisms**: Uses policy-based redaction when AI fails
- **Comprehensive Logging**: Detailed error tracking and debugging
- **Robust Recovery**: Better handling of partial failures

### Performance Optimizations
- **Parallel Processing**: Redaction across models now runs in parallel (sequential per model). Rephrase across models runs in parallel only after all redactions complete.
- **Efficient Batching**: Optimized batch sizes for different models
- **Token Optimization**: Better token usage and limit management
- **Memory Efficiency**: Improved memory usage patterns

### Timeout Configuration
- **Configurable AI Request Timeout**: `POSTPROCESS_AI_REQUEST_TIMEOUT_MS` (default 140000 ms) used in all OpenAI/Azure code paths.
- **Configurable Bedrock Timeout**: `POSTPROCESS_BEDROCK_REQUEST_TIMEOUT_MS` (default 140000 ms) used for Bedrock.
- **Consistent Messaging**: Timeout errors report exact seconds from the configured value.
- **AbortController**: All providers use AbortController with the configured timeouts.

### Instrumentation
- **Per-call timing logs**: Added `[CALL_AI_TIMING] <provider>/<model> took <ms>` around Azure/OpenAI/Bedrock calls to identify slow batches.
- **Heartbeats**: Added `[CALL_AI_TIMING] <provider>/<model> heartbeat <ms>` every 15s during in-flight AI calls to confirm progress before completion or timeout.

### Rate Limiting & Pacing
- **RPM-based pacing**: Implemented per-model RPM pacing via `enforceRpmDelay` using `rpm_limit` from `model_configurations`. Ensures a minimum interval between calls to the same provider/model. Logs `[RPM] <provider>/<model> waiting <ms> (rpm=<limit>)` when delaying.

### Routing and Logging
- **RoutingMode Support**: Backend respects `routingMode` and adjudicated flags to route each comment to the correct provider/model (Scan A vs Scan B) for both phases; evenly splits when both flagged.
- **Detailed Chunk Logs**: Added per-chunk token logs showing input (incl. prompt), estimated output for redaction and rephrase using I/O ratios, and model limits used for gating.

### Conservative Limits Across Models
- **Cross-Model Conservative Limits**: Batch building and AI `max_tokens` now use the most conservative (lowest) input and output token limits across all routed models in a run to ensure uniform safety regardless of grouping.

### Input/Output Improvements
- **Better Format Handling**: Improved parsing of AI responses
- **ID Tagging Support**: Enhanced support for ID-tagged responses
- **Result Validation**: Better validation of transformation results
- **Error Recovery**: Improved handling of malformed responses

### Integration Enhancements
- **Seamless Integration**: Better integration with scan-comments and adjudicator
- **Shared Infrastructure**: Uses same authentication and configuration systems
- **Consistent Logging**: Unified logging and monitoring approach
- **Data Consistency**: Maintains consistency across the processing pipeline

### Monitoring and Logging
- **AI Interaction Logging**: Complete logging of all AI transformations
- **Performance Metrics**: Detailed timing and efficiency measurements
- **Error Correlation**: Better error tracking with scan run IDs
- **Usage Analytics**: Comprehensive usage and performance analytics
