2025-09-12
- Fix alignment bug for ID-tagged AI outputs: when mapping redaction/rephrase results back to comments, coerce `originalRow`/`scannedIndex` to numbers before comparison. This prevents misses when IDs arrive as strings (e.g., "10") while comments hold numbers (10), which could show original text instead of redacted for some rows.
- Frontend merge now preserves backend `finalText` and `mode` fields so UI can reliably display AI-processed text even if `redactedText`/`rephrasedText` are omitted in some routes.

Files impacted:
- supabase/functions/post-process-comments/index.ts
- src/components/CommentEditor.tsx
# Post-Process Comments Edge Function - Recent Changes

## Latest Updates

### Enhanced Content Transformation
- **AI-Powered Redaction**: Improved AI models for intelligent content redaction
- **Smart Rephrasing**: Better preservation of meaning while ensuring anonymity
- **Policy Enforcement**: Enhanced automatic detection and replacement of sensitive content
- **Consistent Output**: Better formatting and structure preservation

### Improved Batch Processing
- **I/O Ratio Optimization**: Batch sizing now uses per-phase I/O ratios from `batch_sizing_config` (`redaction_io_ratio`, `rephrase_io_ratio`).
- **Token-Aware Greedy Chunking**: Implemented greedy chunk builder that respects input token limits (minus prompt reserve) and output token limits per phase, splitting chunks when needed.
- **Dual-Phase Guard**: When both redaction and rephrase are requested, chunks are further split to ensure each phase stays within its output token limit.
- **Safety Margins**: Configurable safety margin percentage applied to calculated batch sizes and to conservative input/output limits and AI call `max_tokens` to reduce overrun risk.
- **Large Dataset Support**: Improved handling of large comment volumes with stable chunking.
- **Timeout Protection**: Standardized timeouts via `POSTPROCESS_AI_REQUEST_TIMEOUT_MS` (default 140000 ms) and `POSTPROCESS_BEDROCK_REQUEST_TIMEOUT_MS` (default 140000 ms).

### Enhanced AI Integration
- **Multiple Provider Support**: Enhanced support for OpenAI and Azure OpenAI
- **Model Configuration**: Uses same configuration system as other functions
- **Temperature Control**: Support for configurable temperature settings
- **Token Limit Optimization**: Better token usage and limit management

### Policy Enforcement Improvements
- **Job Level Detection**: Enhanced detection of job levels, grades, and bands
- **Tenure Redaction**: Better identification and replacement of tenure statements
- **Role Identifier Removal**: Improved detection of role and position identifiers
- **Consistent Placeholders**: Standardized "XXXX" usage across all redactions

### Error Handling Enhancements
- **Graceful Degradation**: Continues processing even when some AI calls fail
- **Fallback Mechanisms**: Uses policy-based redaction when AI fails
- **Comprehensive Logging**: Detailed error tracking and debugging
- **Robust Recovery**: Better handling of partial failures

### Performance Optimizations
- **Parallel Processing**: Redaction and rephrasing run concurrently
- **Efficient Batching**: Optimized batch sizes for different models
- **Token Optimization**: Better token usage and limit management
- **Memory Efficiency**: Improved memory usage patterns

### Timeout Configuration
- **Configurable AI Request Timeout**: `POSTPROCESS_AI_REQUEST_TIMEOUT_MS` (default 140000 ms) used in all OpenAI/Azure code paths.
- **Configurable Bedrock Timeout**: `POSTPROCESS_BEDROCK_REQUEST_TIMEOUT_MS` (default 140000 ms) used for Bedrock.
- **Consistent Messaging**: Timeout errors report exact seconds from the configured value.
- **AbortController**: All providers use AbortController with the configured timeouts.

### Instrumentation
- **Per-call timing logs**: Added `[CALL_AI_TIMING] <provider>/<model> took <ms>` around Azure/OpenAI/Bedrock calls to identify slow batches.
- **Heartbeats**: Added `[CALL_AI_TIMING] <provider>/<model> heartbeat <ms>` every 15s during in-flight AI calls to confirm progress before completion or timeout.

### Rate Limiting & Pacing
- **RPM-based pacing**: Implemented per-model RPM pacing via `enforceRpmDelay` using `rpm_limit` from `model_configurations`. Ensures a minimum interval between calls to the same provider/model. Logs `[RPM] <provider>/<model> waiting <ms> (rpm=<limit>)` when delaying.

### Routing and Logging
- **RoutingMode Support**: Backend respects `routingMode` and adjudicated flags to route each comment to the correct provider/model (Scan A vs Scan B) for both phases; evenly splits when both flagged.
- **Detailed Chunk Logs**: Added per-chunk token logs showing input (incl. prompt), estimated output for redaction and rephrase using I/O ratios, and model limits used for gating.

### Conservative Limits Across Models
- **Cross-Model Conservative Limits**: Batch building and AI `max_tokens` now use the most conservative (lowest) input and output token limits across all routed models in a run to ensure uniform safety regardless of grouping.

### Input/Output Improvements
- **Better Format Handling**: Improved parsing of AI responses
- **ID Tagging Support**: Enhanced support for ID-tagged responses
- **Result Validation**: Better validation of transformation results
- **Error Recovery**: Improved handling of malformed responses

### Integration Enhancements
- **Seamless Integration**: Better integration with scan-comments and adjudicator
- **Shared Infrastructure**: Uses same authentication and configuration systems
- **Consistent Logging**: Unified logging and monitoring approach
- **Data Consistency**: Maintains consistency across the processing pipeline

### Monitoring and Logging
- **AI Interaction Logging**: Complete logging of all AI transformations
- **Performance Metrics**: Detailed timing and efficiency measurements
- **Error Correlation**: Better error tracking with scan run IDs
- **Usage Analytics**: Comprehensive usage and performance analytics
